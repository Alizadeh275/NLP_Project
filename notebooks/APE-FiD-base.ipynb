{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning APE-FiD-base project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIXCEmFrJNg3",
    "outputId": "8d331a24-fc71-433b-a931-f1018b482727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'APE' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/uclnlp/APE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "izf0BHhmwonP",
    "outputId": "444ae08a-c147-421d-85ec-7f01215036e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning FiD-base project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJ1ZlU3FJztQ",
    "outputId": "c569e819-d533-4490-eb2d-2508ee2e0bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'FiD' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/FiD.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to download APE data (Forbidden!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3AxkEoe000j",
    "outputId": "7d3a1814-6dac-4d26-bfcd-09acdc86183a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-02 14:26:23--  http://dl.fbaipublicfiles.com/FiD/preprocessed_data/nq/nq_dpr_train.json.xz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:80... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2022-02-02 14:26:24 ERROR 403: Forbidden.\n",
      "\n",
      "xz: nq_dpr_train.json.xz: No such file or directory\n",
      "--2022-02-02 14:26:24--  http://dl.fbaipublicfiles.com/FiD/preprocessed_data/nq/nq_dpr_dev.json.xz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:80... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2022-02-02 14:26:24 ERROR 403: Forbidden.\n",
      "\n",
      "xz: nq_dpr_dev.json.xz: No such file or directory\n",
      "--2022-02-02 14:26:24--  http://dl.fbaipublicfiles.com/FiD/preprocessed_data/nq/nq_dpr_test.json.xz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:80... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2022-02-02 14:26:25 ERROR 403: Forbidden.\n",
      "\n",
      "xz: nq_dpr_test.json.xz: No such file or directory\n",
      "--2022-02-02 14:26:25--  http://dl.fbaipublicfiles.com/FiD/preprocessed_data/trivia/trivia_dpr_train.json.xz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:80... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2022-02-02 14:26:25 ERROR 403: Forbidden.\n",
      "\n",
      "xz: trivia_dpr_train.json.xz: No such file or directory\n",
      "--2022-02-02 14:26:25--  http://dl.fbaipublicfiles.com/FiD/preprocessed_data/trivia/trivia_dpr_dev.json.xz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:80... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2022-02-02 14:26:25 ERROR 403: Forbidden.\n",
      "\n",
      "xz: trivia_dpr_dev.json.xz: No such file or directory\n",
      "--2022-02-02 14:26:25--  http://dl.fbaipublicfiles.com/FiD/preprocessed_data/trivia/trivia_dpr_test.json.xz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:80... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2022-02-02 14:26:26 ERROR 403: Forbidden.\n",
      "\n",
      "xz: trivia_dpr_test.json.xz: No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "bash APE/scripts/download_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download FiD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "78FNWW2SKKAS",
    "outputId": "225d7947-4c92-4f31-8025-8585d83b2732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data in open_domain_data\n",
      "Downloading Wikipedia passages\n",
      "--2022-02-01 14:34:24--  https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4694541059 (4.4G) [application/gzip]\n",
      "Saving to: ‘open_domain_data/download/psgs_w100.tsv.gz’\n",
      "\n",
      "psgs_w100.tsv.gz    100%[===================>]   4.37G  25.2MB/s    in 3m 27s  \n",
      "\n",
      "2022-02-01 14:37:53 (21.6 MB/s) - ‘open_domain_data/download/psgs_w100.tsv.gz’ saved [4694541059/4694541059]\n",
      "\n",
      "Decompressing Wikipedia passages\n",
      "--2022-02-01 14:42:31--  https://raw.githubusercontent.com/google-research-datasets/natural-questions/master/nq_open/NQ-open.dev.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 391316 (382K) [text/plain]\n",
      "Saving to: ‘open_domain_data/download/NQ-open.dev.jsonl’\n",
      "\n",
      "NQ-open.dev.jsonl   100%[===================>] 382.14K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2022-02-01 14:42:32 (9.12 MB/s) - ‘open_domain_data/download/NQ-open.dev.jsonl’ saved [391316/391316]\n",
      "\n",
      "--2022-02-01 14:42:32--  https://raw.githubusercontent.com/google-research-datasets/natural-questions/master/nq_open/NQ-open.train.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8522298 (8.1M) [text/plain]\n",
      "Saving to: ‘open_domain_data/download/NQ-open.train.jsonl’\n",
      "\n",
      "NQ-open.train.jsonl 100%[===================>]   8.13M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-02-01 14:42:33 (79.1 MB/s) - ‘open_domain_data/download/NQ-open.train.jsonl’ saved [8522298/8522298]\n",
      "\n",
      "--2022-02-01 14:42:33--  http://nlp.cs.washington.edu/triviaqa/data/triviaqa-unfiltered.tar.gz\n",
      "Resolving nlp.cs.washington.edu (nlp.cs.washington.edu)... 128.208.3.120, 2607:4000:200:12::78\n",
      "Connecting to nlp.cs.washington.edu (nlp.cs.washington.edu)|128.208.3.120|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 632549060 (603M) [application/x-gzip]\n",
      "Saving to: ‘open_domain_data/download/triviaqa-unfiltered.tar.gz’\n",
      "\n",
      "triviaqa-unfiltered 100%[===================>] 603.25M  14.3MB/s    in 83s     \n",
      "\n",
      "2022-02-01 14:43:57 (7.24 MB/s) - ‘open_domain_data/download/triviaqa-unfiltered.tar.gz’ saved [632549060/632549060]\n",
      "\n",
      "triviaqa-unfiltered/\n",
      "triviaqa-unfiltered/unfiltered-web-train.json\n",
      "triviaqa-unfiltered/README\n",
      "triviaqa-unfiltered/unfiltered-web-dev.json\n",
      "triviaqa-unfiltered/unfiltered-web-test-without-answers.json\n",
      "--2022-02-01 14:44:37--  https://dl.fbaipublicfiles.com/FiD/data/dataindex.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 483821 (472K) [application/gzip]\n",
      "Saving to: ‘open_domain_data/download/dataindex.tar.gz’\n",
      "\n",
      "dataindex.tar.gz    100%[===================>] 472.48K   734KB/s    in 0.6s    \n",
      "\n",
      "2022-02-01 14:44:40 (734 KB/s) - ‘open_domain_data/download/dataindex.tar.gz’ saved [483821/483821]\n",
      "\n",
      "NQ.dev.idx.json\n",
      "NQ.test.idx.json\n",
      "NQ.train.idx.json\n",
      "TQA.dev.idx.json\n",
      "TQA.test.idx.json\n",
      "TQA.train.idx.json\n",
      "--2022-02-01 14:44:40--  https://dl.fbaipublicfiles.com/FiD/data/nq_passages.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 35243587 (34M) [application/gzip]\n",
      "Saving to: ‘open_domain_data/download/nq_passages.tar.gz’\n",
      "\n",
      "nq_passages.tar.gz  100%[===================>]  33.61M  14.5MB/s    in 2.3s    \n",
      "\n",
      "2022-02-01 14:44:43 (14.5 MB/s) - ‘open_domain_data/download/nq_passages.tar.gz’ saved [35243587/35243587]\n",
      "\n",
      "--2022-02-01 14:44:43--  https://dl.fbaipublicfiles.com/FiD/data/tqa_passages.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40106382 (38M) [application/gzip]\n",
      "Saving to: ‘open_domain_data/download/tqa_passages.tar.gz’\n",
      "\n",
      "tqa_passages.tar.gz 100%[===================>]  38.25M  12.1MB/s    in 3.2s    \n",
      "\n",
      "2022-02-01 14:44:47 (12.1 MB/s) - ‘open_domain_data/download/tqa_passages.tar.gz’ saved [40106382/40106382]\n",
      "\n",
      "tqa_passages/\n",
      "tqa_passages/dev.json\n",
      "tqa_passages/test.json\n",
      "tqa_passages/train.json\n",
      "nq_passages/\n",
      "nq_passages/dev.json\n",
      "nq_passages/test.json\n",
      "nq_passages/train.json\n",
      "Processing open_domain_data\n",
      "python3: can't open file 'src/preprocess.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-24e255875732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bash FiD/get-data.sh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 139\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'bash FiD/get-data.sh' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "%%shell\n",
    "bash FiD/get-data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "NoRUEihjRaxT",
    "outputId": "b913cd64-e6c8-43a7-8aa6-ad8cbdb6b32b"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a2df1d9aed13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m#append data to csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m           chunksize=rowsize)#size of data to append for each loop\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[1;32m   3168\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m         )\n\u001b[0;32m-> 3170\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m             )\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    360\u001b[0m         )\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mlibwriters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mpandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We need just one part, so after generating first part we stopped this cell\n",
    "import pandas as pd\n",
    "\n",
    "in_csv = '/content/open_domain_data/psgs_w100.tsv'\n",
    "\n",
    "number_lines = sum(1 for row in (open(in_csv)))\n",
    "\n",
    "rowsize = round(number_lines / 8)\n",
    "\n",
    "for i in range(1,number_lines,rowsize):\n",
    "\n",
    "    df = pd.read_csv(in_csv, header=None, nrows = rowsize, skiprows = i , delimiter='\\t')\n",
    "    out_csv = 'input' + str(i) + '.csv'\n",
    "\n",
    "    df.to_csv(out_csv,\n",
    "          index=False,\n",
    "          header=False,\n",
    "          mode='a',#append data to csv file\n",
    "          chunksize=rowsize)#size of data to append for each loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgUz2uA69gm6",
    "outputId": "d159bce1-ee65-4a3a-bc5e-b611be2f09d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "\n",
    "rm -r \"open_domain_data/psgs_w100.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7_IY24hNied",
    "outputId": "0aa46678-c4d8-4317-f0e1-80b4163577b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing open_domain_data\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "ROOT=\"open_domain_data\"\n",
    "DOWNLOAD=$ROOT/\"download\"\n",
    "\n",
    "\n",
    "echo \"Processing \"\"$ROOT\"\n",
    "python FiD/src/preprocess.py $DOWNLOAD $ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcQjIdN1_63p"
   },
   "outputs": [],
   "source": [
    "# from FiD.src import util\n",
    "# passages = util.load_passages('input1.csv')\n",
    "# passages = {p[0]: (p[1], p[2]) for p in passages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnWnguEeAec1",
    "outputId": "b404f0cf-cc05-424a-ba95-584c031268d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# if 123234 in passages:\n",
    "#   print(1)\n",
    "# else:\n",
    "#   print(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install requierments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Id_n4G_f_8l",
    "outputId": "56e524b1-f429-45eb-d8c1-7a3a114b7026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r APE/requirements.txt (line 1)) (2.7.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r APE/requirements.txt (line 2)) (4.62.3)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from -r APE/requirements.txt (line 3)) (0.1.96)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (1.43.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (3.17.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (57.4.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r APE/requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard->-r APE/requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r APE/requirements.txt (line 1)) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r APE/requirements.txt (line 1)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r APE/requirements.txt (line 1)) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r APE/requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->-r APE/requirements.txt (line 1)) (4.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r APE/requirements.txt (line 1)) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r APE/requirements.txt (line 1)) (3.10.0.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r APE/requirements.txt (line 1)) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r APE/requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r APE/requirements.txt (line 1)) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r APE/requirements.txt (line 1)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r APE/requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r APE/requirements.txt (line 1)) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r APE/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tzVeHeq0G12f",
    "outputId": "679d141d-1d22-4340-a78c-7d235e69467c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.0.2\n",
      "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "\u001b[K     |████████████████████████████████| 769 kB 4.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.62.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 28.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.4.2)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 39.7 MB/s \n",
      "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
      "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 28.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
      "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.8.1rc1 transformers-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==3.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove checkpoint folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IwahGBDHNbo",
    "outputId": "f30975f1-cc13-4dd8-f214-7734d7175570"
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "\n",
    "rm -r \"checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YkyPHBSQaWX",
    "outputId": "07262351-e208-4d43-8570-e13d957dc27f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100% 792k/792k [00:00<00:00, 870kB/s]\n",
      "tcmalloc: large alloc 1363296256 bytes == 0x5621509aa000 @  0x7fe13b13a1e7 0x562146ac3f98 0x562146a8ee27 0x562146c0d115 0x562146ba7888 0x562146a926f2 0x562146b70c6e 0x562146ba7802 0x562146a926f2 0x562146a95926 0x562146c107a3 0x562146a92349 0x562146b83e1d 0x562146b05e99 0x562146b00ced 0x562146a93bda 0x562146b05d00 0x562146b009ee 0x562146a93bda 0x562146b02737 0x562146b009ee 0x562146b006f3 0x562146bca4c2 0x562146bca83d 0x562146bca6e6 0x562146ba2163 0x562146ba1e0c 0x7fe139f24bf7 0x562146ba1cea\n",
      "tcmalloc: large alloc 1363296256 bytes == 0x5621a1dce000 @  0x7fe13b13a1e7 0x562146ac3f98 0x562146ad94ec 0x562146b66e93 0x562146a92349 0x562146a92240 0x562146b060f3 0x562146b009ee 0x562146a94271 0x562146a93720 0x562146a95698 0x562146b72441 0x562146c107d1 0x562146a92349 0x562146b83e1d 0x562146b05e99 0x562146b00ced 0x562146a93bda 0x562146b05d00 0x562146b009ee 0x562146a93bda 0x562146b02737 0x562146b009ee 0x562146b006f3 0x562146bca4c2 0x562146bca83d 0x562146bca6e6 0x562146ba2163 0x562146ba1e0c 0x7fe139f24bf7 0x562146ba1cea\n",
      "Downloading: 100% 1.20k/1.20k [00:00<00:00, 878kB/s]\n",
      "Downloading: 100% 892M/892M [00:50<00:00, 17.6MB/s]\n",
      "[02/01/2022 15:31:37] {modeling_utils.py:768} WARNING - Some weights of FiDT5 were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "10000it [3:24:43,  1.23s/it] \n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "\n",
    "python APE/FiD/train.py \\\n",
    "  --checkpoint_dir checkpoint \\\n",
    "  --train_data_path open_domain_data/NQ/train.json \\\n",
    "  --dev_data_path open_domain_data/NQ/dev.json \\\n",
    "  --model_size base \\\n",
    "  --per_gpu_batch_size 4 \\\n",
    "  --n_context 1 \\\n",
    "  --name my_experiment \\\n",
    "  --eval_freq 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nDx9R0hm4cZ",
    "outputId": "9d43cbc0-a0fc-47a7-995b-1ab5e8a2cdc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "[02/01/2022 19:59:47] {configuration_utils.py:262} INFO - loading configuration file pretrained_models/nq_reader_base/config.json\n",
      "[02/01/2022 19:59:47] {configuration_utils.py:300} INFO - Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"FiDT5\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[02/01/2022 19:59:47] {modeling_utils.py:665} INFO - loading weights file pretrained_models/nq_reader_base/pytorch_model.bin\n",
      "[02/01/2022 19:59:54] {modeling_utils.py:757} WARNING - Some weights of the model checkpoint at pretrained_models/nq_reader_base were not used when initializing FiDT5: ['encoder.encoder.embed_tokens.weight', 'encoder.encoder.block.0.module.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.0.module.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.0.module.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.0.module.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.0.module.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.encoder.block.0.module.layer.0.layer_norm.weight', 'encoder.encoder.block.0.module.layer.1.DenseReluDense.wi.weight', 'encoder.encoder.block.0.module.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.0.module.layer.1.layer_norm.weight', 'encoder.encoder.block.1.module.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.1.module.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.1.module.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.1.module.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.1.module.layer.0.layer_norm.weight', 'encoder.encoder.block.1.module.layer.1.DenseReluDense.wi.weight', 'encoder.encoder.block.1.module.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.1.module.layer.1.layer_norm.weight', 'encoder.encoder.block.2.module.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.2.module.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.2.module.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.2.module.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.2.module.layer.0.layer_norm.weight', 'encoder.encoder.block.2.module.layer.1.DenseReluDense.wi.weight', 'encoder.encoder.block.2.module.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.2.module.layer.1.layer_norm.weight', 'encoder.encoder.block.3.module.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.3.module.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.3.module.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.3.module.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.3.module.layer.0.layer_norm.weight', 'encoder.encoder.block.3.module.layer.1.DenseReluDense.wi.weight', 'encoder.encoder.block.3.module.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.3.module.layer.1.layer_norm.weight', 'encoder.encoder.block.4.module.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.4.module.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.4.module.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.4.module.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.4.module.layer.0.layer_norm.weight', 'encoder.encoder.block.4.module.layer.1.DenseReluDense.wi.weight', 'encoder.encoder.block.4.module.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.4.module.layer.1.layer_norm.weight', 'encoder.encoder.block.5.module.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.5.module.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.5.module.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.5.module.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.5.module.layer.0.layer_norm.weight', 'encoder.encoder.block.5.module.layer.1.DenseReluDense.wi.weight', 'encoder.encoder.block.5.module.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.5.module.layer.1.layer_norm.weight', 'encoder.encoder.block.6.module.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.6.module.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.6.module.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.6.module.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.6.module.layer.0.layer_norm.weight', 'encoder.encoder.block.6.module.layer.1.DenseReluDense.wi.weight', 'encoder.encoder.block.6.module.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.6.module.layer.1.layer_norm.weight', 'encoder.encoder.block.7.module.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.7.module.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.7.module.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.7.module.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.7.module.layer.0.layer_norm.weight', 'encoder.encoder.block.7.module.layer.1.DenseReluDense.wi.weight', 'encoder.encoder.block.7.module.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.7.module.layer.1.layer_norm.weight', 'encoder.encoder.block.8.module.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.8.module.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.8.module.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.8.module.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.8.module.layer.0.layer_norm.weight', 'encoder.encoder.block.8.module.layer.1.DenseReluDense.wi.weight', 'encoder.encoder.block.8.module.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.8.module.layer.1.layer_norm.weight', 'encoder.encoder.block.9.module.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.9.module.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.9.module.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.9.module.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.9.module.layer.0.layer_norm.weight', 'encoder.encoder.block.9.module.layer.1.DenseReluDense.wi.weight', 'encoder.encoder.block.9.module.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.9.module.layer.1.layer_norm.weight', 'encoder.encoder.block.10.module.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.10.module.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.10.module.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.10.module.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.10.module.layer.0.layer_norm.weight', 'encoder.encoder.block.10.module.layer.1.DenseReluDense.wi.weight', 'encoder.encoder.block.10.module.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.10.module.layer.1.layer_norm.weight', 'encoder.encoder.block.11.module.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.11.module.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.11.module.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.11.module.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.11.module.layer.0.layer_norm.weight', 'encoder.encoder.block.11.module.layer.1.DenseReluDense.wi.weight', 'encoder.encoder.block.11.module.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.11.module.layer.1.layer_norm.weight', 'encoder.encoder.final_layer_norm.weight']\n",
      "- This IS expected if you are initializing FiDT5 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing FiDT5 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[02/01/2022 19:59:54] {modeling_utils.py:768} WARNING - Some weights of FiDT5 were not initialized from the model checkpoint at pretrained_models/nq_reader_base and are newly initialized: ['encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[02/01/2022 19:59:57] {test.py:127} INFO - Start eval\n",
      "[02/01/2022 20:22:01] {test.py:65} INFO - total number of example 3610\n",
      "[02/01/2022 20:22:01] {test.py:136} INFO - EM 39.128542\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "\n",
    "\n",
    "python APE/FiD/test.py \\\n",
    "  --model_path pretrained_models/nq_reader_base \\\n",
    "  --test_data_path open_domain_data/NQ/test.json \\\n",
    "  --model_size base \\\n",
    "  --per_gpu_batch_size 4 \\\n",
    "  --n_context 1 \\\n",
    "  --name my_test \\\n",
    "  --checkpoint_dir checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUolg2UU0qoq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
